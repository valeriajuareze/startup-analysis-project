{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report, log_loss\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import psycopg2 \n",
    "import pandas.io.sql as sqlio \n",
    "from password import db_password\n",
    "connection = psycopg2.connect(user='postgres', password=db_password, \n",
    "host='127.0.0.1',\n",
    "port='5432', \n",
    "database='Startup-Analysis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = \"SELECT * FROM startup_alldata\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sqlio.read_sql_query(sql,connection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state_code</th>\n",
       "      <th>year_</th>\n",
       "      <th>vp_gdp</th>\n",
       "      <th>labels</th>\n",
       "      <th>founded_at</th>\n",
       "      <th>age_first_funding_year</th>\n",
       "      <th>age_last_funding_year</th>\n",
       "      <th>age_first_milestone_year</th>\n",
       "      <th>age_last_milestone_year</th>\n",
       "      <th>relationships</th>\n",
       "      <th>...</th>\n",
       "      <th>has_angel</th>\n",
       "      <th>has_rounda</th>\n",
       "      <th>has_roundb</th>\n",
       "      <th>has_roundc</th>\n",
       "      <th>has_roundd</th>\n",
       "      <th>avg_participants</th>\n",
       "      <th>is_top500</th>\n",
       "      <th>reached_milestone</th>\n",
       "      <th>founded_first_funding_days_difference</th>\n",
       "      <th>first_last_funding_days_difference</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CA</td>\n",
       "      <td>2007</td>\n",
       "      <td>1.9</td>\n",
       "      <td>1</td>\n",
       "      <td>2007</td>\n",
       "      <td>2.2493</td>\n",
       "      <td>3.0027</td>\n",
       "      <td>4.6685</td>\n",
       "      <td>6.7041</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>821</td>\n",
       "      <td>275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CA</td>\n",
       "      <td>2000</td>\n",
       "      <td>7.7</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>5.1260</td>\n",
       "      <td>9.9973</td>\n",
       "      <td>7.0055</td>\n",
       "      <td>7.0055</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4.7500</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1871</td>\n",
       "      <td>1778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CA</td>\n",
       "      <td>2009</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1</td>\n",
       "      <td>2009</td>\n",
       "      <td>1.0329</td>\n",
       "      <td>1.0329</td>\n",
       "      <td>1.4575</td>\n",
       "      <td>2.2055</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>377</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CA</td>\n",
       "      <td>2002</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1</td>\n",
       "      <td>2002</td>\n",
       "      <td>3.1315</td>\n",
       "      <td>5.3151</td>\n",
       "      <td>6.0027</td>\n",
       "      <td>6.0027</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3.3333</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1143</td>\n",
       "      <td>797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CA</td>\n",
       "      <td>2010</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2010</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.6685</td>\n",
       "      <td>0.0384</td>\n",
       "      <td>0.0384</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>903</th>\n",
       "      <td>WA</td>\n",
       "      <td>2000</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>5.6301</td>\n",
       "      <td>8.7589</td>\n",
       "      <td>8.5041</td>\n",
       "      <td>8.7589</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5.6000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2055</td>\n",
       "      <td>1142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>904</th>\n",
       "      <td>CA</td>\n",
       "      <td>2009</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1</td>\n",
       "      <td>2009</td>\n",
       "      <td>0.5178</td>\n",
       "      <td>0.5178</td>\n",
       "      <td>0.5808</td>\n",
       "      <td>4.5260</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>189</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>905</th>\n",
       "      <td>CA</td>\n",
       "      <td>1999</td>\n",
       "      <td>7.2</td>\n",
       "      <td>0</td>\n",
       "      <td>1999</td>\n",
       "      <td>8.4959</td>\n",
       "      <td>8.4959</td>\n",
       "      <td>9.0055</td>\n",
       "      <td>9.0055</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>8.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3101</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>906</th>\n",
       "      <td>CA</td>\n",
       "      <td>2009</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1</td>\n",
       "      <td>2009</td>\n",
       "      <td>0.7589</td>\n",
       "      <td>2.8329</td>\n",
       "      <td>0.7589</td>\n",
       "      <td>3.8356</td>\n",
       "      <td>12</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>277</td>\n",
       "      <td>757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>907</th>\n",
       "      <td>CA</td>\n",
       "      <td>2003</td>\n",
       "      <td>3.9</td>\n",
       "      <td>1</td>\n",
       "      <td>2003</td>\n",
       "      <td>3.1205</td>\n",
       "      <td>3.1205</td>\n",
       "      <td>4.0027</td>\n",
       "      <td>4.0027</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1139</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>908 rows Ã— 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    state_code  year_  vp_gdp  labels  founded_at  age_first_funding_year  \\\n",
       "0           CA   2007     1.9       1        2007                  2.2493   \n",
       "1           CA   2000     7.7       1        2000                  5.1260   \n",
       "2           CA   2009     3.2       1        2009                  1.0329   \n",
       "3           CA   2002     1.8       1        2002                  3.1315   \n",
       "4           CA   2010     2.0       0        2010                  0.0000   \n",
       "..         ...    ...     ...     ...         ...                     ...   \n",
       "903         WA   2000     0.7       1        2000                  5.6301   \n",
       "904         CA   2009     3.2       1        2009                  0.5178   \n",
       "905         CA   1999     7.2       0        1999                  8.4959   \n",
       "906         CA   2009     3.2       1        2009                  0.7589   \n",
       "907         CA   2003     3.9       1        2003                  3.1205   \n",
       "\n",
       "     age_last_funding_year  age_first_milestone_year  age_last_milestone_year  \\\n",
       "0                   3.0027                    4.6685                   6.7041   \n",
       "1                   9.9973                    7.0055                   7.0055   \n",
       "2                   1.0329                    1.4575                   2.2055   \n",
       "3                   5.3151                    6.0027                   6.0027   \n",
       "4                   1.6685                    0.0384                   0.0384   \n",
       "..                     ...                       ...                      ...   \n",
       "903                 8.7589                    8.5041                   8.7589   \n",
       "904                 0.5178                    0.5808                   4.5260   \n",
       "905                 8.4959                    9.0055                   9.0055   \n",
       "906                 2.8329                    0.7589                   3.8356   \n",
       "907                 3.1205                    4.0027                   4.0027   \n",
       "\n",
       "     relationships  ...  has_angel  has_rounda  has_roundb  has_roundc  \\\n",
       "0                3  ...          1           0           0           0   \n",
       "1                9  ...          0           0           1           1   \n",
       "2                5  ...          0           1           0           0   \n",
       "3                5  ...          0           0           1           1   \n",
       "4                2  ...          1           0           0           0   \n",
       "..             ...  ...        ...         ...         ...         ...   \n",
       "903              9  ...          0           0           0           1   \n",
       "904              9  ...          0           1           0           0   \n",
       "905              5  ...          0           0           0           0   \n",
       "906             12  ...          0           1           1           0   \n",
       "907              4  ...          0           0           1           0   \n",
       "\n",
       "     has_roundd  avg_participants  is_top500  reached_milestone  \\\n",
       "0             0            1.0000          0                  1   \n",
       "1             1            4.7500          1                  1   \n",
       "2             0            4.0000          1                  1   \n",
       "3             1            3.3333          1                  1   \n",
       "4             0            1.0000          1                  1   \n",
       "..          ...               ...        ...                ...   \n",
       "903           1            5.6000          1                  1   \n",
       "904           0            6.0000          1                  1   \n",
       "905           1            8.0000          1                  1   \n",
       "906           0            1.0000          1                  1   \n",
       "907           0            3.0000          1                  1   \n",
       "\n",
       "     founded_first_funding_days_difference  first_last_funding_days_difference  \n",
       "0                                      821                                 275  \n",
       "1                                     1871                                1778  \n",
       "2                                      377                                   0  \n",
       "3                                     1143                                 797  \n",
       "4                                        0                                 609  \n",
       "..                                     ...                                 ...  \n",
       "903                                   2055                                1142  \n",
       "904                                    189                                   0  \n",
       "905                                   3101                                   0  \n",
       "906                                    277                                 757  \n",
       "907                                   1139                                   0  \n",
       "\n",
       "[908 rows x 39 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data[\"age_first_milestone_year\"] = data[\"age_first_milestone_year\"].fillna(value=0)\n",
    "data[\"age_last_milestone_year\"] = data[\"age_last_milestone_year\"].fillna(value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop(\"labels\", \"\", axis= 1)\n",
    "y = data[\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Create train and transform data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'CA'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_35472\\1053671501.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# fit the data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mX_scaler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# Transform data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\OMEN\\anaconda3\\envs\\mlenv\\lib\\site-packages\\sklearn\\preprocessing\\_data.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    804\u001b[0m         \u001b[1;31m# Reset internal state before fitting\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    805\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 806\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpartial_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    807\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    808\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpartial_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\OMEN\\anaconda3\\envs\\mlenv\\lib\\site-packages\\sklearn\\preprocessing\\_data.py\u001b[0m in \u001b[0;36mpartial_fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    845\u001b[0m             \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFLOAT_DTYPES\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    846\u001b[0m             \u001b[0mforce_all_finite\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"allow-nan\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 847\u001b[1;33m             \u001b[0mreset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfirst_call\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    848\u001b[0m         )\n\u001b[0;32m    849\u001b[0m         \u001b[0mn_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\OMEN\\anaconda3\\envs\\mlenv\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    559\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Validation should be done on X, y or both.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    560\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 561\u001b[1;33m             \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    562\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    563\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\OMEN\\anaconda3\\envs\\mlenv\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[0;32m    736\u001b[0m                     \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcasting\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"unsafe\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    737\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 738\u001b[1;33m                     \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    739\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcomplex_warning\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    740\u001b[0m                 raise ValueError(\n",
      "\u001b[1;32mc:\\Users\\OMEN\\anaconda3\\envs\\mlenv\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m__array__\u001b[1;34m(self, dtype)\u001b[0m\n\u001b[0;32m   1991\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1992\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__array__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mNpDtype\u001b[0m \u001b[1;33m|\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1993\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1994\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1995\u001b[0m     def __array_wrap__(\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: 'CA'"
     ]
    }
   ],
   "source": [
    "### Scale the data \n",
    "scaler = StandardScaler()\n",
    "\n",
    "# fit the data\n",
    "X_scaler = scaler.fit(X_train)\n",
    "\n",
    "# Transform data\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create 2 models without PCA\n",
    "\n",
    "#Start with logistic regression \n",
    "model = LogisticRegression(solver=\"lbfgs\", random_state=42)\n",
    "model.fit(X_train_scaled, y_train)\n",
    "predictions = model.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7575757575757576\n",
      "[[ 49  36]\n",
      " [ 20 126]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.58      0.64        85\n",
      "           1       0.78      0.86      0.82       146\n",
      "\n",
      "    accuracy                           0.76       231\n",
      "   macro avg       0.74      0.72      0.73       231\n",
      "weighted avg       0.75      0.76      0.75       231\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print Scores\n",
    "print(accuracy_score(y_test,predictions))\n",
    "print(confusion_matrix(y_test,predictions))\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model = RandomForestClassifier(n_estimators=200, random_state=42)\n",
    "rf_model.fit(X_train_scaled, y_train)\n",
    "y_pred = rf_model.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8138528138528138\n",
      "[[ 50  35]\n",
      " [  8 138]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.59      0.70        85\n",
      "           1       0.80      0.95      0.87       146\n",
      "\n",
      "    accuracy                           0.81       231\n",
      "   macro avg       0.83      0.77      0.78       231\n",
      "weighted avg       0.82      0.81      0.80       231\n",
      "\n",
      "6.429417190005267\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_test,y_pred))\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(log_loss(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cumulative Variance Ratio</th>\n",
       "      <th>Explained Variance Ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.136897</td>\n",
       "      <td>0.136897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.246626</td>\n",
       "      <td>0.109729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.309848</td>\n",
       "      <td>0.063222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.363830</td>\n",
       "      <td>0.053982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.406120</td>\n",
       "      <td>0.042290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.446112</td>\n",
       "      <td>0.039992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.484111</td>\n",
       "      <td>0.037999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.519461</td>\n",
       "      <td>0.035350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.553890</td>\n",
       "      <td>0.034430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.587272</td>\n",
       "      <td>0.033382</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Cumulative Variance Ratio  Explained Variance Ratio\n",
       "0                   0.136897                  0.136897\n",
       "1                   0.246626                  0.109729\n",
       "2                   0.309848                  0.063222\n",
       "3                   0.363830                  0.053982\n",
       "4                   0.406120                  0.042290\n",
       "5                   0.446112                  0.039992\n",
       "6                   0.484111                  0.037999\n",
       "7                   0.519461                  0.035350\n",
       "8                   0.553890                  0.034430\n",
       "9                   0.587272                  0.033382"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### Create same 2 models with PCA transformation\n",
    "pca_test = PCA(n_components=34)\n",
    "pca_test.fit(X_train_scaled)\n",
    "sns.set(style='whitegrid')\n",
    "plt.plot(np.cumsum(pca_test.explained_variance_ratio_))\n",
    "plt.xlabel('number of components')\n",
    "plt.ylabel('cumulative explained variance')\n",
    "plt.axvline(linewidth=4, color='r', linestyle = '--', x=20, ymin=0, ymax=1)\n",
    "display(plt.show())\n",
    "evr = pca_test.explained_variance_ratio_\n",
    "cvr = np.cumsum(pca_test.explained_variance_ratio_)\n",
    "pca_df = pd.DataFrame()\n",
    "pca_df['Cumulative Variance Ratio'] = cvr\n",
    "pca_df['Explained Variance Ratio'] = evr\n",
    "pca_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components= 20)\n",
    "pca.fit(X_train_scaled)\n",
    "\n",
    "X_train_scaled_pca = pca.transform(X_train_scaled)\n",
    "X_test_scaled_pca = pca.transform(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators= 200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf.fit(X_train_scaled_pca, y_train)\n",
    "y_pred= rf.predict(X_test_scaled_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7272727272727273\n",
      "[[ 42  43]\n",
      " [ 20 126]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.49      0.57        85\n",
      "           1       0.75      0.86      0.80       146\n",
      "\n",
      "    accuracy                           0.73       231\n",
      "   macro avg       0.71      0.68      0.69       231\n",
      "weighted avg       0.72      0.73      0.72       231\n",
      "\n",
      "9.419815132332776\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_test,y_pred))\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(log_loss(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic Regression\n",
    "log_model = LogisticRegression(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_model.fit(X_train_scaled_pca, y_train)\n",
    "predictions = log_model.predict(X_test_scaled_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7575757575757576\n",
      "[[ 50  35]\n",
      " [ 21 125]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.59      0.64        85\n",
      "           1       0.78      0.86      0.82       146\n",
      "\n",
      "    accuracy                           0.76       231\n",
      "   macro avg       0.74      0.72      0.73       231\n",
      "weighted avg       0.75      0.76      0.75       231\n",
      "\n",
      "8.373157852922317\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_test,predictions))\n",
    "print(confusion_matrix(y_test,predictions))\n",
    "print(classification_report(y_test, predictions))\n",
    "print(log_loss(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_model = tf.keras.models.Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_model.add(tf.keras.layers.Dense(units=80, activation=\"relu\", input_dim=34))\n",
    "nn_model.add(tf.keras.layers.Dense(units=50, activation=\"relu\"))\n",
    "nn_model.add(tf.keras.layers.Dense(units=30, activation=\"tanh\"))\n",
    "nn_model.add(tf.keras.layers.Dropout(0.5))\n",
    "nn_model.add(tf.keras.layers.Dense(units=20, activation=\"relu\"))\n",
    "nn_model.add(tf.keras.layers.Dense(units=1, activation=\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_23 (Dense)            (None, 80)                2800      \n",
      "                                                                 \n",
      " dense_24 (Dense)            (None, 50)                4050      \n",
      "                                                                 \n",
      " dense_25 (Dense)            (None, 30)                1530      \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 30)                0         \n",
      "                                                                 \n",
      " dense_26 (Dense)            (None, 20)                620       \n",
      "                                                                 \n",
      " dense_27 (Dense)            (None, 1)                 21        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 9,021\n",
      "Trainable params: 9,021\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "nn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "22/22 [==============================] - 1s 3ms/step - loss: 0.6447 - accuracy: 0.6460\n",
      "Epoch 2/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.5732 - accuracy: 0.7095\n",
      "Epoch 3/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.5422 - accuracy: 0.7428\n",
      "Epoch 4/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.5153 - accuracy: 0.7645\n",
      "Epoch 5/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.5152 - accuracy: 0.7630\n",
      "Epoch 6/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.4770 - accuracy: 0.7803\n",
      "Epoch 7/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4716 - accuracy: 0.7803\n",
      "Epoch 8/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.4391 - accuracy: 0.8020\n",
      "Epoch 9/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.4371 - accuracy: 0.8150\n",
      "Epoch 10/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.4238 - accuracy: 0.8150\n",
      "Epoch 11/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.3996 - accuracy: 0.8223\n",
      "Epoch 12/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.3940 - accuracy: 0.8367\n",
      "Epoch 13/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.3688 - accuracy: 0.8540\n",
      "Epoch 14/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.3413 - accuracy: 0.8526\n",
      "Epoch 15/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.3271 - accuracy: 0.8743\n",
      "Epoch 16/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.3232 - accuracy: 0.8699\n",
      "Epoch 17/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.2924 - accuracy: 0.8772\n",
      "Epoch 18/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.2802 - accuracy: 0.8829\n",
      "Epoch 19/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.2521 - accuracy: 0.9075\n",
      "Epoch 20/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.2311 - accuracy: 0.9205\n",
      "Epoch 21/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.2168 - accuracy: 0.9191\n",
      "Epoch 22/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.1967 - accuracy: 0.9277\n",
      "Epoch 23/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.1845 - accuracy: 0.9277\n",
      "Epoch 24/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.1720 - accuracy: 0.9408\n",
      "Epoch 25/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.1431 - accuracy: 0.9581\n",
      "Epoch 26/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.1257 - accuracy: 0.9639\n",
      "Epoch 27/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.1225 - accuracy: 0.9610\n",
      "Epoch 28/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.1086 - accuracy: 0.9595\n",
      "Epoch 29/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0939 - accuracy: 0.9697\n",
      "Epoch 30/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0818 - accuracy: 0.9754\n",
      "Epoch 31/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0748 - accuracy: 0.9783\n",
      "Epoch 32/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0774 - accuracy: 0.9711\n",
      "Epoch 33/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0789 - accuracy: 0.9783\n",
      "Epoch 34/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0644 - accuracy: 0.9812\n",
      "Epoch 35/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0640 - accuracy: 0.9783\n",
      "Epoch 36/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0558 - accuracy: 0.9841\n",
      "Epoch 37/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0560 - accuracy: 0.9899\n",
      "Epoch 38/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0417 - accuracy: 0.9884\n",
      "Epoch 39/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0360 - accuracy: 0.9870\n",
      "Epoch 40/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0353 - accuracy: 0.9899\n",
      "Epoch 41/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0269 - accuracy: 0.9957\n",
      "Epoch 42/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0279 - accuracy: 0.9928\n",
      "Epoch 43/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0198 - accuracy: 0.9971\n",
      "Epoch 44/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0301 - accuracy: 0.9913\n",
      "Epoch 45/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0254 - accuracy: 0.9913\n",
      "Epoch 46/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0415 - accuracy: 0.9899\n",
      "Epoch 47/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0187 - accuracy: 0.9986\n",
      "Epoch 48/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0204 - accuracy: 0.9942\n",
      "Epoch 49/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0221 - accuracy: 0.9913\n",
      "Epoch 50/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0316 - accuracy: 0.9928\n",
      "Epoch 51/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0316 - accuracy: 0.9928\n",
      "Epoch 52/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0179 - accuracy: 0.9957\n",
      "Epoch 53/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0163 - accuracy: 0.9957\n",
      "Epoch 54/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0242 - accuracy: 0.9942\n",
      "Epoch 55/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0179 - accuracy: 0.9971\n",
      "Epoch 56/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0116 - accuracy: 0.9971\n",
      "Epoch 57/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0134 - accuracy: 0.9971\n",
      "Epoch 58/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0178 - accuracy: 0.9928\n",
      "Epoch 59/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0235 - accuracy: 0.9942\n",
      "Epoch 60/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0109 - accuracy: 0.9971\n",
      "Epoch 61/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0082 - accuracy: 0.9986\n",
      "Epoch 62/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0149 - accuracy: 0.9942\n",
      "Epoch 63/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0347 - accuracy: 0.9855\n",
      "Epoch 64/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0330 - accuracy: 0.9913\n",
      "Epoch 65/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0183 - accuracy: 0.9913\n",
      "Epoch 66/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0304 - accuracy: 0.9855\n",
      "Epoch 67/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0427 - accuracy: 0.9870\n",
      "Epoch 68/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0244 - accuracy: 0.9928\n",
      "Epoch 69/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0099 - accuracy: 0.9986\n",
      "Epoch 70/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0045 - accuracy: 1.0000\n",
      "Epoch 71/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0055 - accuracy: 0.9986\n",
      "Epoch 72/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0086 - accuracy: 0.9971\n",
      "Epoch 73/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0206 - accuracy: 0.9928\n",
      "Epoch 74/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0117 - accuracy: 0.9957\n",
      "Epoch 75/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0046 - accuracy: 0.9986\n",
      "Epoch 76/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0040 - accuracy: 1.0000\n",
      "Epoch 77/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0062 - accuracy: 0.9986\n",
      "Epoch 78/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0047 - accuracy: 0.9986\n",
      "Epoch 79/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0024 - accuracy: 1.0000\n",
      "Epoch 80/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0029 - accuracy: 1.0000\n",
      "Epoch 81/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0044 - accuracy: 0.9986\n",
      "Epoch 82/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0081 - accuracy: 0.9971\n",
      "Epoch 83/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0081 - accuracy: 0.9971\n",
      "Epoch 84/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0029 - accuracy: 1.0000\n",
      "Epoch 85/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0096 - accuracy: 0.9986\n",
      "Epoch 86/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0097 - accuracy: 0.9971\n",
      "Epoch 87/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0176 - accuracy: 0.9971\n",
      "Epoch 88/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0038 - accuracy: 1.0000\n",
      "Epoch 89/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0139 - accuracy: 0.9928\n",
      "Epoch 90/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0152 - accuracy: 0.9971\n",
      "Epoch 91/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0044 - accuracy: 0.9986\n",
      "Epoch 92/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0054 - accuracy: 0.9986\n",
      "Epoch 93/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0066 - accuracy: 0.9971\n",
      "Epoch 94/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0089 - accuracy: 0.9986\n",
      "Epoch 95/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0020 - accuracy: 1.0000\n",
      "Epoch 96/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0018 - accuracy: 1.0000\n",
      "Epoch 97/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0031 - accuracy: 1.0000\n",
      "Epoch 98/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0048 - accuracy: 0.9986\n",
      "Epoch 99/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0035 - accuracy: 0.9986\n",
      "Epoch 100/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0023 - accuracy: 0.9986\n",
      "Epoch 101/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.0041 - accuracy: 0.9986\n",
      "Epoch 102/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0017 - accuracy: 1.0000\n",
      "Epoch 103/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0043 - accuracy: 0.9986\n",
      "Epoch 104/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0139 - accuracy: 0.9986\n",
      "Epoch 105/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0045 - accuracy: 0.9986\n",
      "Epoch 106/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 107/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0133 - accuracy: 0.9986\n",
      "Epoch 108/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0041 - accuracy: 0.9986\n",
      "Epoch 109/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0202 - accuracy: 0.9971\n",
      "Epoch 110/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0407 - accuracy: 0.9942\n",
      "Epoch 111/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.1126 - accuracy: 0.9725\n",
      "Epoch 112/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0985 - accuracy: 0.9769\n",
      "Epoch 113/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0631 - accuracy: 0.9769\n",
      "Epoch 114/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0729 - accuracy: 0.9754\n",
      "Epoch 115/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0384 - accuracy: 0.9899\n",
      "Epoch 116/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0177 - accuracy: 0.9928\n",
      "Epoch 117/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0175 - accuracy: 0.9957\n",
      "Epoch 118/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0107 - accuracy: 0.9971\n",
      "Epoch 119/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0059 - accuracy: 0.9986\n",
      "Epoch 120/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0035 - accuracy: 1.0000\n",
      "Epoch 121/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0022 - accuracy: 1.0000\n",
      "Epoch 122/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0029 - accuracy: 1.0000\n",
      "Epoch 123/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0021 - accuracy: 1.0000\n",
      "Epoch 124/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0021 - accuracy: 1.0000\n",
      "Epoch 125/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0020 - accuracy: 1.0000\n",
      "Epoch 126/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 127/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 128/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 129/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 130/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 131/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 132/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.0018 - accuracy: 1.0000\n",
      "Epoch 133/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 134/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 9.8132e-04 - accuracy: 1.0000\n",
      "Epoch 135/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 8.6366e-04 - accuracy: 1.0000\n",
      "Epoch 136/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 7.8442e-04 - accuracy: 1.0000\n",
      "Epoch 137/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 9.2230e-04 - accuracy: 1.0000\n",
      "Epoch 138/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 8.0978e-04 - accuracy: 1.0000\n",
      "Epoch 139/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 7.3754e-04 - accuracy: 1.0000\n",
      "Epoch 140/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 6.7339e-04 - accuracy: 1.0000\n",
      "Epoch 141/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 5.8931e-04 - accuracy: 1.0000\n",
      "Epoch 142/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 8.7893e-04 - accuracy: 1.0000\n",
      "Epoch 143/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 144/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 8.6432e-04 - accuracy: 1.0000\n",
      "Epoch 145/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 4.3676e-04 - accuracy: 1.0000\n",
      "Epoch 146/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 8.4344e-04 - accuracy: 1.0000\n",
      "Epoch 147/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 4.4729e-04 - accuracy: 1.0000\n",
      "Epoch 148/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 7.7541e-04 - accuracy: 1.0000\n",
      "Epoch 149/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 4.2881e-04 - accuracy: 1.0000\n",
      "Epoch 150/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 3.4140e-04 - accuracy: 1.0000\n",
      "Epoch 151/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 4.1413e-04 - accuracy: 1.0000\n",
      "Epoch 152/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 5.1131e-04 - accuracy: 1.0000\n",
      "Epoch 153/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 4.7427e-04 - accuracy: 1.0000\n",
      "Epoch 154/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 4.4071e-04 - accuracy: 1.0000\n",
      "Epoch 155/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 5.5558e-04 - accuracy: 1.0000\n",
      "Epoch 156/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 6.4785e-04 - accuracy: 1.0000\n",
      "Epoch 157/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 7.7120e-04 - accuracy: 1.0000\n",
      "Epoch 158/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 5.4169e-04 - accuracy: 1.0000\n",
      "Epoch 159/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 4.1357e-04 - accuracy: 1.0000\n",
      "Epoch 160/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 3.9672e-04 - accuracy: 1.0000\n",
      "Epoch 161/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 4.7321e-04 - accuracy: 1.0000\n",
      "Epoch 162/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 5.4061e-04 - accuracy: 1.0000\n",
      "Epoch 163/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 3.1323e-04 - accuracy: 1.0000\n",
      "Epoch 164/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 4.7013e-04 - accuracy: 1.0000\n",
      "Epoch 165/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 4.3004e-04 - accuracy: 1.0000\n",
      "Epoch 166/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 5.2939e-04 - accuracy: 1.0000\n",
      "Epoch 167/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 3.3894e-04 - accuracy: 1.0000\n",
      "Epoch 168/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 4.1448e-04 - accuracy: 1.0000\n",
      "Epoch 169/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 3.9645e-04 - accuracy: 1.0000\n",
      "Epoch 170/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 3.2628e-04 - accuracy: 1.0000\n",
      "Epoch 171/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 2.3437e-04 - accuracy: 1.0000\n",
      "Epoch 172/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 2.8581e-04 - accuracy: 1.0000\n",
      "Epoch 173/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 3.5239e-04 - accuracy: 1.0000\n",
      "Epoch 174/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 3.4194e-04 - accuracy: 1.0000\n",
      "Epoch 175/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 2.0900e-04 - accuracy: 1.0000\n",
      "Epoch 176/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 1.9592e-04 - accuracy: 1.0000\n",
      "Epoch 177/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 2.3953e-04 - accuracy: 1.0000\n",
      "Epoch 178/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 2.5694e-04 - accuracy: 1.0000\n",
      "Epoch 179/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.5417e-04 - accuracy: 1.0000\n",
      "Epoch 180/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 3.1786e-04 - accuracy: 1.0000\n",
      "Epoch 181/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 2.3900e-04 - accuracy: 1.0000\n",
      "Epoch 182/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 1.6411e-04 - accuracy: 1.0000\n",
      "Epoch 183/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.4104e-04 - accuracy: 1.0000\n",
      "Epoch 184/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 1.6406e-04 - accuracy: 1.0000\n",
      "Epoch 185/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 2.3928e-04 - accuracy: 1.0000\n",
      "Epoch 186/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.7787e-04 - accuracy: 1.0000\n",
      "Epoch 187/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 2.2865e-04 - accuracy: 1.0000\n",
      "Epoch 188/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.5730e-04 - accuracy: 1.0000\n",
      "Epoch 189/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 2.2412e-04 - accuracy: 1.0000\n",
      "Epoch 190/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 2.4317e-04 - accuracy: 1.0000\n",
      "Epoch 191/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 2.0399e-04 - accuracy: 1.0000\n",
      "Epoch 192/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.9078e-04 - accuracy: 1.0000\n",
      "Epoch 193/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 3.0727e-04 - accuracy: 1.0000\n",
      "Epoch 194/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 2.0457e-04 - accuracy: 1.0000\n",
      "Epoch 195/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.4453e-04 - accuracy: 1.0000\n",
      "Epoch 196/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.8408e-04 - accuracy: 1.0000\n",
      "Epoch 197/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.2586e-04 - accuracy: 1.0000\n",
      "Epoch 198/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.4547e-04 - accuracy: 1.0000\n",
      "Epoch 199/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.6520e-04 - accuracy: 1.0000\n",
      "Epoch 200/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.8582e-04 - accuracy: 1.0000\n",
      "Epoch 201/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 2.9681e-04 - accuracy: 1.0000\n",
      "Epoch 202/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 2.7694e-04 - accuracy: 1.0000\n",
      "Epoch 203/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 5.2065e-04 - accuracy: 1.0000\n",
      "Epoch 204/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 4.7367e-04 - accuracy: 1.0000\n",
      "Epoch 205/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 4.8962e-04 - accuracy: 1.0000\n",
      "Epoch 206/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 2.7717e-04 - accuracy: 1.0000\n",
      "Epoch 207/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 2.0957e-04 - accuracy: 1.0000\n",
      "Epoch 208/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 1.0625e-04 - accuracy: 1.0000\n",
      "Epoch 209/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 2.5060e-04 - accuracy: 1.0000\n",
      "Epoch 210/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 3.5874e-04 - accuracy: 1.0000\n",
      "Epoch 211/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 1.5076e-04 - accuracy: 1.0000\n",
      "Epoch 212/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.0155 - accuracy: 0.9942\n",
      "Epoch 213/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.3323 - accuracy: 0.9451\n",
      "Epoch 214/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.1770 - accuracy: 0.9480\n",
      "Epoch 215/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.1043 - accuracy: 0.9610\n",
      "Epoch 216/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0783 - accuracy: 0.9711\n",
      "Epoch 217/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0182 - accuracy: 0.9971\n",
      "Epoch 218/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0115 - accuracy: 1.0000\n",
      "Epoch 219/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0072 - accuracy: 1.0000\n",
      "Epoch 220/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0051 - accuracy: 1.0000\n",
      "Epoch 221/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0047 - accuracy: 0.9986\n",
      "Epoch 222/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0046 - accuracy: 1.0000\n",
      "Epoch 223/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.0043 - accuracy: 0.9986\n",
      "Epoch 224/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.0050 - accuracy: 1.0000\n",
      "Epoch 225/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.0035 - accuracy: 1.0000\n",
      "Epoch 226/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0048 - accuracy: 0.9986\n",
      "Epoch 227/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0064 - accuracy: 0.9986\n",
      "Epoch 228/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0129 - accuracy: 0.9986\n",
      "Epoch 229/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0018 - accuracy: 1.0000\n",
      "Epoch 230/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0047 - accuracy: 0.9986\n",
      "Epoch 231/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0080 - accuracy: 0.9986\n",
      "Epoch 232/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0033 - accuracy: 0.9986\n",
      "Epoch 233/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 234/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 235/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 8.0065e-04 - accuracy: 1.0000\n",
      "Epoch 236/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 9.7478e-04 - accuracy: 1.0000\n",
      "Epoch 237/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 9.8027e-04 - accuracy: 1.0000\n",
      "Epoch 238/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 239/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 9.0525e-04 - accuracy: 1.0000\n",
      "Epoch 240/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 7.9767e-04 - accuracy: 1.0000\n",
      "Epoch 241/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 242/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 6.5475e-04 - accuracy: 1.0000\n",
      "Epoch 243/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 9.4438e-04 - accuracy: 1.0000\n",
      "Epoch 244/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 6.6751e-04 - accuracy: 1.0000\n",
      "Epoch 245/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 9.2929e-04 - accuracy: 1.0000\n",
      "Epoch 246/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 6.1467e-04 - accuracy: 1.0000\n",
      "Epoch 247/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 6.8248e-04 - accuracy: 1.0000\n",
      "Epoch 248/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 6.6611e-04 - accuracy: 1.0000\n",
      "Epoch 249/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 5.1991e-04 - accuracy: 1.0000\n",
      "Epoch 250/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 5.4108e-04 - accuracy: 1.0000\n",
      "Epoch 251/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 5.2844e-04 - accuracy: 1.0000\n",
      "Epoch 252/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 6.4227e-04 - accuracy: 1.0000\n",
      "Epoch 253/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 4.0829e-04 - accuracy: 1.0000\n",
      "Epoch 254/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 3.5946e-04 - accuracy: 1.0000\n",
      "Epoch 255/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 4.6486e-04 - accuracy: 1.0000\n",
      "Epoch 256/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 4.9495e-04 - accuracy: 1.0000\n",
      "Epoch 257/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 3.2012e-04 - accuracy: 1.0000\n",
      "Epoch 258/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 5.6468e-04 - accuracy: 1.0000\n",
      "Epoch 259/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 3.0494e-04 - accuracy: 1.0000\n",
      "Epoch 260/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 3.0578e-04 - accuracy: 1.0000\n",
      "Epoch 261/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 3.6623e-04 - accuracy: 1.0000\n",
      "Epoch 262/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 2.8522e-04 - accuracy: 1.0000\n",
      "Epoch 263/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 3.8635e-04 - accuracy: 1.0000\n",
      "Epoch 264/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 4.3652e-04 - accuracy: 1.0000\n",
      "Epoch 265/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 3.4539e-04 - accuracy: 1.0000\n",
      "Epoch 266/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 2.9283e-04 - accuracy: 1.0000\n",
      "Epoch 267/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 3.5634e-04 - accuracy: 1.0000\n",
      "Epoch 268/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 2.5352e-04 - accuracy: 1.0000\n",
      "Epoch 269/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 3.7979e-04 - accuracy: 1.0000\n",
      "Epoch 270/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 2.6664e-04 - accuracy: 1.0000\n",
      "Epoch 271/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 3.1184e-04 - accuracy: 1.0000\n",
      "Epoch 272/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.9147e-04 - accuracy: 1.0000\n",
      "Epoch 273/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 2.4645e-04 - accuracy: 1.0000\n",
      "Epoch 274/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 3.2618e-04 - accuracy: 1.0000\n",
      "Epoch 275/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 2.9199e-04 - accuracy: 1.0000\n",
      "Epoch 276/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1.7139e-04 - accuracy: 1.0000\n",
      "Epoch 277/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 2.7731e-04 - accuracy: 1.0000\n",
      "Epoch 278/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 2.3729e-04 - accuracy: 1.0000\n",
      "Epoch 279/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 1.7683e-04 - accuracy: 1.0000\n",
      "Epoch 280/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 3.5446e-04 - accuracy: 1.0000\n",
      "Epoch 281/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1.9402e-04 - accuracy: 1.0000\n",
      "Epoch 282/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 2.1729e-04 - accuracy: 1.0000\n",
      "Epoch 283/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 2.8123e-04 - accuracy: 1.0000\n",
      "Epoch 284/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 1.8205e-04 - accuracy: 1.0000\n",
      "Epoch 285/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 2.1940e-04 - accuracy: 1.0000\n",
      "Epoch 286/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 1.6132e-04 - accuracy: 1.0000\n",
      "Epoch 287/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.3845e-04 - accuracy: 1.0000\n",
      "Epoch 288/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 2.1418e-04 - accuracy: 1.0000\n",
      "Epoch 289/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 1.3374e-04 - accuracy: 1.0000\n",
      "Epoch 290/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.7709e-04 - accuracy: 1.0000\n",
      "Epoch 291/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.7045e-04 - accuracy: 1.0000\n",
      "Epoch 292/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 2.6952e-04 - accuracy: 1.0000\n",
      "Epoch 293/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 1.6310e-04 - accuracy: 1.0000\n",
      "Epoch 294/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 2.1418e-04 - accuracy: 1.0000\n",
      "Epoch 295/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0016 - accuracy: 0.9986\n",
      "Epoch 296/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0261 - accuracy: 0.9957\n",
      "Epoch 297/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0255 - accuracy: 0.9942\n",
      "Epoch 298/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0760 - accuracy: 0.9783\n",
      "Epoch 299/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0967 - accuracy: 0.9740\n",
      "Epoch 300/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0704 - accuracy: 0.9798\n",
      "Epoch 301/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0958 - accuracy: 0.9711\n",
      "Epoch 302/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0529 - accuracy: 0.9798\n",
      "Epoch 303/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.0318 - accuracy: 0.9957\n",
      "Epoch 304/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0128 - accuracy: 0.9986\n",
      "Epoch 305/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0045 - accuracy: 1.0000\n",
      "Epoch 306/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.0037 - accuracy: 1.0000\n",
      "Epoch 307/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0037 - accuracy: 1.0000\n",
      "Epoch 308/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0024 - accuracy: 1.0000\n",
      "Epoch 309/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0023 - accuracy: 1.0000\n",
      "Epoch 310/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0019 - accuracy: 1.0000\n",
      "Epoch 311/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0018 - accuracy: 1.0000\n",
      "Epoch 312/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 313/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0019 - accuracy: 1.0000\n",
      "Epoch 314/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 315/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 316/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 317/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 318/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 319/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 8.2054e-04 - accuracy: 1.0000\n",
      "Epoch 320/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 7.1723e-04 - accuracy: 1.0000\n",
      "Epoch 321/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 6.9966e-04 - accuracy: 1.0000\n",
      "Epoch 322/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 5.6841e-04 - accuracy: 1.0000\n",
      "Epoch 323/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 7.1868e-04 - accuracy: 1.0000\n",
      "Epoch 324/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 5.4948e-04 - accuracy: 1.0000\n",
      "Epoch 325/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 3.9039e-04 - accuracy: 1.0000\n",
      "Epoch 326/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 4.2712e-04 - accuracy: 1.0000\n",
      "Epoch 327/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 6.1845e-04 - accuracy: 1.0000\n",
      "Epoch 328/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 4.8880e-04 - accuracy: 1.0000\n",
      "Epoch 329/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 5.4553e-04 - accuracy: 1.0000\n",
      "Epoch 330/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 4.1242e-04 - accuracy: 1.0000\n",
      "Epoch 331/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 4.0619e-04 - accuracy: 1.0000\n",
      "Epoch 332/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 5.6138e-04 - accuracy: 1.0000\n",
      "Epoch 333/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 4.1239e-04 - accuracy: 1.0000\n",
      "Epoch 334/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 3.1110e-04 - accuracy: 1.0000\n",
      "Epoch 335/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 4.1910e-04 - accuracy: 1.0000\n",
      "Epoch 336/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 5.9385e-04 - accuracy: 1.0000\n",
      "Epoch 337/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 3.6449e-04 - accuracy: 1.0000\n",
      "Epoch 338/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 4.4794e-04 - accuracy: 1.0000\n",
      "Epoch 339/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 2.7829e-04 - accuracy: 1.0000\n",
      "Epoch 340/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 2.1966e-04 - accuracy: 1.0000\n",
      "Epoch 341/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 3.8819e-04 - accuracy: 1.0000\n",
      "Epoch 342/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 3.3182e-04 - accuracy: 1.0000\n",
      "Epoch 343/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 2.7926e-04 - accuracy: 1.0000\n",
      "Epoch 344/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 3.3962e-04 - accuracy: 1.0000\n",
      "Epoch 345/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 4.9258e-04 - accuracy: 1.0000\n",
      "Epoch 346/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 2.2131e-04 - accuracy: 1.0000\n",
      "Epoch 347/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 2.4246e-04 - accuracy: 1.0000\n",
      "Epoch 348/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 2.4687e-04 - accuracy: 1.0000\n",
      "Epoch 349/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 1.8572e-04 - accuracy: 1.0000\n",
      "Epoch 350/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 2.1602e-04 - accuracy: 1.0000\n",
      "Epoch 351/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 2.4774e-04 - accuracy: 1.0000\n",
      "Epoch 352/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 3.6125e-04 - accuracy: 1.0000\n",
      "Epoch 353/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 2.2497e-04 - accuracy: 1.0000\n",
      "Epoch 354/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 1.7245e-04 - accuracy: 1.0000\n",
      "Epoch 355/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1.6492e-04 - accuracy: 1.0000\n",
      "Epoch 356/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 2.8369e-04 - accuracy: 1.0000\n",
      "Epoch 357/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 1.9086e-04 - accuracy: 1.0000\n",
      "Epoch 358/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 1.6519e-04 - accuracy: 1.0000\n",
      "Epoch 359/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 3.1639e-04 - accuracy: 1.0000\n",
      "Epoch 360/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 3.3916e-04 - accuracy: 1.0000\n",
      "Epoch 361/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.9360e-04 - accuracy: 1.0000\n",
      "Epoch 362/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 2.1027e-04 - accuracy: 1.0000\n",
      "Epoch 363/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.1079e-04 - accuracy: 1.0000\n",
      "Epoch 364/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 4.0213e-04 - accuracy: 1.0000\n",
      "Epoch 365/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.7768e-04 - accuracy: 1.0000\n",
      "Epoch 366/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.6474e-04 - accuracy: 1.0000\n",
      "Epoch 367/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.9627e-04 - accuracy: 1.0000\n",
      "Epoch 368/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 3.6076e-04 - accuracy: 1.0000\n",
      "Epoch 369/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 2.5184e-04 - accuracy: 1.0000\n",
      "Epoch 370/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.1222e-04 - accuracy: 1.0000\n",
      "Epoch 371/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 2.4438e-04 - accuracy: 1.0000\n",
      "Epoch 372/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 2.0863e-04 - accuracy: 1.0000\n",
      "Epoch 373/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.2177e-04 - accuracy: 1.0000\n",
      "Epoch 374/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.4994e-04 - accuracy: 1.0000\n",
      "Epoch 375/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.5577e-04 - accuracy: 1.0000\n",
      "Epoch 376/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.0445e-04 - accuracy: 1.0000\n",
      "Epoch 377/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 8.5522e-05 - accuracy: 1.0000\n",
      "Epoch 378/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.4418e-04 - accuracy: 1.0000\n",
      "Epoch 379/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.5080e-04 - accuracy: 1.0000\n",
      "Epoch 380/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.0459e-04 - accuracy: 1.0000\n",
      "Epoch 381/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.0883e-04 - accuracy: 1.0000\n",
      "Epoch 382/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 1.9807e-04 - accuracy: 1.0000\n",
      "Epoch 383/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 1.2808e-04 - accuracy: 1.0000\n",
      "Epoch 384/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 9.8921e-05 - accuracy: 1.0000\n",
      "Epoch 385/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.1946e-04 - accuracy: 1.0000\n",
      "Epoch 386/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 7.3367e-05 - accuracy: 1.0000\n",
      "Epoch 387/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 1.1215e-04 - accuracy: 1.0000\n",
      "Epoch 388/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 1.1312e-04 - accuracy: 1.0000\n",
      "Epoch 389/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 1.0965e-04 - accuracy: 1.0000\n",
      "Epoch 390/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 1.0732e-04 - accuracy: 1.0000\n",
      "Epoch 391/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 8.8644e-05 - accuracy: 1.0000\n",
      "Epoch 392/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 7.8883e-05 - accuracy: 1.0000\n",
      "Epoch 393/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 1.3878e-04 - accuracy: 1.0000\n",
      "Epoch 394/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 7.4287e-05 - accuracy: 1.0000\n",
      "Epoch 395/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 8.2460e-05 - accuracy: 1.0000\n",
      "Epoch 396/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 9.3423e-05 - accuracy: 1.0000\n",
      "Epoch 397/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 6.2616e-05 - accuracy: 1.0000\n",
      "Epoch 398/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.4942e-04 - accuracy: 1.0000\n",
      "Epoch 399/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 8.6883e-05 - accuracy: 1.0000\n",
      "Epoch 400/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.5085e-04 - accuracy: 1.0000\n",
      "Epoch 401/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 7.2038e-05 - accuracy: 1.0000\n",
      "Epoch 402/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 6.0018e-05 - accuracy: 1.0000\n",
      "Epoch 403/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 5.7853e-05 - accuracy: 1.0000\n",
      "Epoch 404/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.0150e-04 - accuracy: 1.0000\n",
      "Epoch 405/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 1.0993e-04 - accuracy: 1.0000\n",
      "Epoch 406/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 9.4455e-05 - accuracy: 1.0000\n",
      "Epoch 407/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 6.0949e-05 - accuracy: 1.0000\n",
      "Epoch 408/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.9738e-04 - accuracy: 1.0000\n",
      "Epoch 409/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 6.9288e-05 - accuracy: 1.0000\n",
      "Epoch 410/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 5.6713e-05 - accuracy: 1.0000\n",
      "Epoch 411/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.2186e-04 - accuracy: 1.0000\n",
      "Epoch 412/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 4.9342e-05 - accuracy: 1.0000\n",
      "Epoch 413/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 7.1938e-05 - accuracy: 1.0000\n",
      "Epoch 414/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 5.6622e-05 - accuracy: 1.0000\n",
      "Epoch 415/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 6.1552e-05 - accuracy: 1.0000\n",
      "Epoch 416/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 5.6457e-05 - accuracy: 1.0000\n",
      "Epoch 417/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 4.1258e-05 - accuracy: 1.0000\n",
      "Epoch 418/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 3.7674e-05 - accuracy: 1.0000\n",
      "Epoch 419/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 6.7012e-05 - accuracy: 1.0000\n",
      "Epoch 420/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 8.4440e-05 - accuracy: 1.0000\n",
      "Epoch 421/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 4.9402e-05 - accuracy: 1.0000\n",
      "Epoch 422/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 3.6865e-05 - accuracy: 1.0000\n",
      "Epoch 423/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 6.9346e-05 - accuracy: 1.0000\n",
      "Epoch 424/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 5.5897e-05 - accuracy: 1.0000\n",
      "Epoch 425/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 2.6544e-05 - accuracy: 1.0000\n",
      "Epoch 426/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 8.6538e-05 - accuracy: 1.0000\n",
      "Epoch 427/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 2.4116e-05 - accuracy: 1.0000\n",
      "Epoch 428/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 2.9672e-05 - accuracy: 1.0000\n",
      "Epoch 429/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 2.9723e-05 - accuracy: 1.0000\n",
      "Epoch 430/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 2.4135e-05 - accuracy: 1.0000\n",
      "Epoch 431/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 8.0052e-05 - accuracy: 1.0000\n",
      "Epoch 432/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 4.1336e-05 - accuracy: 1.0000\n",
      "Epoch 433/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 5.6727e-05 - accuracy: 1.0000\n",
      "Epoch 434/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 8.4900e-05 - accuracy: 1.0000\n",
      "Epoch 435/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 6.7388e-05 - accuracy: 1.0000\n",
      "Epoch 436/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 4.0086e-05 - accuracy: 1.0000\n",
      "Epoch 437/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 4.8056e-05 - accuracy: 1.0000\n",
      "Epoch 438/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 6.3545e-05 - accuracy: 1.0000\n",
      "Epoch 439/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 2.1629e-05 - accuracy: 1.0000\n",
      "Epoch 440/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 6.7699e-05 - accuracy: 1.0000\n",
      "Epoch 441/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 3.4161e-05 - accuracy: 1.0000\n",
      "Epoch 442/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 2.3787e-04 - accuracy: 1.0000\n",
      "Epoch 443/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 3.3072e-05 - accuracy: 1.0000\n",
      "Epoch 444/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 5.2246e-05 - accuracy: 1.0000\n",
      "Epoch 445/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 4.9210e-05 - accuracy: 1.0000\n",
      "Epoch 446/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 7.8684e-05 - accuracy: 1.0000\n",
      "Epoch 447/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 4.1681e-05 - accuracy: 1.0000\n",
      "Epoch 448/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 6.4992e-05 - accuracy: 1.0000\n",
      "Epoch 449/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 6.9487e-05 - accuracy: 1.0000\n",
      "Epoch 450/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 4.4608e-05 - accuracy: 1.0000\n",
      "Epoch 451/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 4.0555e-05 - accuracy: 1.0000\n",
      "Epoch 452/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 2.4540e-05 - accuracy: 1.0000\n",
      "Epoch 453/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 3.5405e-05 - accuracy: 1.0000\n",
      "Epoch 454/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 4.9125e-05 - accuracy: 1.0000\n",
      "Epoch 455/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 2.6703e-05 - accuracy: 1.0000\n",
      "Epoch 456/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 6.1850e-05 - accuracy: 1.0000\n",
      "Epoch 457/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 3.0552e-05 - accuracy: 1.0000\n",
      "Epoch 458/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 4.2654e-05 - accuracy: 1.0000\n",
      "Epoch 459/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 3.4195e-05 - accuracy: 1.0000\n",
      "Epoch 460/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 7.3746e-05 - accuracy: 1.0000\n",
      "Epoch 461/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 7.5251e-05 - accuracy: 1.0000\n",
      "Epoch 462/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 2.1624e-05 - accuracy: 1.0000\n",
      "Epoch 463/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.3778e-05 - accuracy: 1.0000\n",
      "Epoch 464/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 8.8844e-05 - accuracy: 1.0000\n",
      "Epoch 465/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 4.6190e-04 - accuracy: 1.0000\n",
      "Epoch 466/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.1063 - accuracy: 0.9841\n",
      "Epoch 467/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.1118 - accuracy: 0.9610\n",
      "Epoch 468/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.1871 - accuracy: 0.9552\n",
      "Epoch 469/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0784 - accuracy: 0.9769\n",
      "Epoch 470/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0257 - accuracy: 0.9942\n",
      "Epoch 471/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0118 - accuracy: 0.9986\n",
      "Epoch 472/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0090 - accuracy: 0.9971\n",
      "Epoch 473/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0046 - accuracy: 1.0000\n",
      "Epoch 474/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0029 - accuracy: 1.0000\n",
      "Epoch 475/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0032 - accuracy: 1.0000\n",
      "Epoch 476/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0020 - accuracy: 1.0000\n",
      "Epoch 477/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0019 - accuracy: 1.0000\n",
      "Epoch 478/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 479/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 480/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 481/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 482/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 8.7692e-04 - accuracy: 1.0000\n",
      "Epoch 483/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 8.0927e-04 - accuracy: 1.0000\n",
      "Epoch 484/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 7.7242e-04 - accuracy: 1.0000\n",
      "Epoch 485/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0010 - accuracy: 1.0000\n",
      "Epoch 486/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 7.6069e-04 - accuracy: 1.0000\n",
      "Epoch 487/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 6.7199e-04 - accuracy: 1.0000\n",
      "Epoch 488/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 6.7935e-04 - accuracy: 1.0000\n",
      "Epoch 489/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 5.3987e-04 - accuracy: 1.0000\n",
      "Epoch 490/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 6.4838e-04 - accuracy: 1.0000\n",
      "Epoch 491/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 5.4697e-04 - accuracy: 1.0000\n",
      "Epoch 492/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 7.8898e-04 - accuracy: 1.0000\n",
      "Epoch 493/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 5.6877e-04 - accuracy: 1.0000\n",
      "Epoch 494/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 4.8014e-04 - accuracy: 1.0000\n",
      "Epoch 495/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 4.3874e-04 - accuracy: 1.0000\n",
      "Epoch 496/500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 3.8927e-04 - accuracy: 1.0000\n",
      "Epoch 497/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 3.6271e-04 - accuracy: 1.0000\n",
      "Epoch 498/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 4.6431e-04 - accuracy: 1.0000\n",
      "Epoch 499/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 3.7178e-04 - accuracy: 1.0000\n",
      "Epoch 500/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 3.7297e-04 - accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "fit_model = nn_model.fit(X_train_scaled, y_train, epochs=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a DataFrame containing training history\n",
    "history_df = pd.DataFrame(fit_model.history, index=range(1,len(fit_model.history[\"loss\"])+1))\n",
    "\n",
    "# Plot the loss\n",
    "history_df.plot(y=\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the accuracy\n",
    "history_df.plot(y=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 - 0s - loss: 1.4052 - accuracy: 0.7403 - 38ms/epoch - 5ms/step\n",
      "Loss: 1.4051733016967773, Accuracy: 0.7402597665786743\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model using the test data\n",
    "model_loss, model_accuracy = nn_model2.evaluate(X_test_scaled,y_test,verbose=2)\n",
    "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The first random forest looks like the best option, the NN is overffiting so we are not going to use it like that, looking at the labels it \\nappears to need a undersampling metodology so we are going to use a Random Undersampling'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"The first random forest looks like the best option, the NN is overffiting so we are not going to use it like that, looking at the labels it \n",
    "appears to need a undersampling metodology so we are going to use a Random Undersampling\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## New resample\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "ros = RandomUnderSampler(random_state=42)\n",
    "X_resampled, y_resampled = ros.fit_resample(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## initializing random forest\n",
    "rf_ = RandomForestClassifier(n_estimators= 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## fitting and predicting\n",
    "rf_.fit(X_resampled, y_resampled)\n",
    "y_pred = rf_.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7489177489177489\n",
      "[[ 62  23]\n",
      " [ 35 111]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.73      0.68        85\n",
      "           1       0.83      0.76      0.79       146\n",
      "\n",
      "    accuracy                           0.75       231\n",
      "   macro avg       0.73      0.74      0.74       231\n",
      "weighted avg       0.76      0.75      0.75       231\n",
      "\n",
      "8.672153340457637\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'got worse precission but better recall, f1-score worse so this is not the solution'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### looking accuracy\n",
    "print(accuracy_score(y_test,y_pred))\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(log_loss(y_test, y_pred))\n",
    "\"\"\"got worse precission but better recall, f1-score worse so this is not the solution\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### creating a rf function\n",
    "def rf_model(X,y,n_estimators):\n",
    "    model = RandomForestClassifier(n_estimators=n_estimators)\n",
    "    ### create split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "    model.fit(X_train, y_train)\n",
    "    predictions = model.predict(X_test)\n",
    "    report= classification_report(y_test,predictions)\n",
    "    return report\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.metrics import classification_report_imbalanced\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier, EasyEnsembleClassifier\n",
    "from imblearn.combine import SMOTEENN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create oversampler function\n",
    "def rf_model(X,y,model):\n",
    "    model = model(n_estimators=200)\n",
    "    ### create split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "    over_sampler = RandomOverSampler(random_state=42)\n",
    "    X_resampled, y_resampled = over_sampler.fit_resample(X_train,y_train)\n",
    "    model.fit(X_resampled, y_resampled)\n",
    "    predictions = model.predict(X_test)\n",
    "    report= classification_report_imbalanced(y_test,predictions)\n",
    "    accuracy = accuracy_score(y_test,predictions)\n",
    "    return print(report), print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   pre       rec       spe        f1       geo       iba       sup\n",
      "\n",
      "          0       0.78      0.62      0.91      0.69      0.75      0.55        82\n",
      "          1       0.81      0.91      0.62      0.86      0.75      0.58       149\n",
      "\n",
      "avg / total       0.80      0.81      0.72      0.80      0.75      0.57       231\n",
      "\n",
      "0.8051948051948052\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_model(X,y, BalancedRandomForestClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   pre       rec       spe        f1       geo       iba       sup\n",
      "\n",
      "          0       0.61      0.62      0.77      0.62      0.69      0.47        85\n",
      "          1       0.78      0.77      0.62      0.77      0.69      0.49       146\n",
      "\n",
      "avg / total       0.72      0.71      0.68      0.71      0.69      0.48       231\n",
      "\n",
      "0.7142857142857143\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_model(X,y,EasyEnsembleClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create oversampler function\n",
    "estimator = [int(x) for x in np.linspace(start=100, stop=1000, num=10)]\n",
    "def rf_model(X,y,model):\n",
    "\n",
    "    model = model(n_estimators=1000)\n",
    "    ### create split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "    smote = SMOTEENN(random_state=42)\n",
    "    X_resampled, y_resampled = smote.fit_resample(X_train,y_train)\n",
    "    model.fit(X_resampled, y_resampled)\n",
    "    predictions = model.predict(X_test)\n",
    "    report= classification_report(y_test,predictions)\n",
    "    accuracy = accuracy_score(y_test,predictions)\n",
    "    return print(report), print(f'the accuracy for {model} is {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.49      0.60      0.54        68\n",
      "           1       0.82      0.74      0.78       163\n",
      "\n",
      "    accuracy                           0.70       231\n",
      "   macro avg       0.66      0.67      0.66       231\n",
      "weighted avg       0.72      0.70      0.71       231\n",
      "\n",
      "the accuracy for RandomForestClassifier(n_estimators=1000) is 0.7012987012987013\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "rf_model(X,y, RandomForestClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.54      0.57        78\n",
      "           1       0.78      0.82      0.80       153\n",
      "\n",
      "    accuracy                           0.72       231\n",
      "   macro avg       0.69      0.68      0.68       231\n",
      "weighted avg       0.72      0.72      0.72       231\n",
      "\n",
      "the accuracy for EasyEnsembleClassifier(n_estimators=1000) is 0.7229437229437229\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_model(X,y,EasyEnsembleClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.60      0.59        80\n",
      "           1       0.79      0.77      0.78       151\n",
      "\n",
      "    accuracy                           0.71       231\n",
      "   macro avg       0.69      0.69      0.69       231\n",
      "weighted avg       0.72      0.71      0.72       231\n",
      "\n",
      "the accuracy for BalancedRandomForestClassifier(n_estimators=1000) is 0.7142857142857143\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_model(X,y, BalancedRandomForestClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.55      0.59        85\n",
      "           1       0.76      0.81      0.78       146\n",
      "\n",
      "    accuracy                           0.71       231\n",
      "   macro avg       0.69      0.68      0.68       231\n",
      "weighted avg       0.71      0.71      0.71       231\n",
      "\n",
      "the accuracy for RandomForestClassifier(n_estimators=1000) is 0.7142857142857143\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_model(X,y, RandomForestClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_model = SVC(kernel=\"sigmoid\", probability= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(kernel='sigmoid', probability=True)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_model.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = svm_model.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7619047619047619"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.67      0.67        85\n",
      "           1       0.81      0.82      0.81       146\n",
      "\n",
      "    accuracy                           0.76       231\n",
      "   macro avg       0.74      0.74      0.74       231\n",
      "weighted avg       0.76      0.76      0.76       231\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.62      0.62        87\n",
      "           1       0.77      0.78      0.78       144\n",
      "\n",
      "    accuracy                           0.72       231\n",
      "   macro avg       0.70      0.70      0.70       231\n",
      "weighted avg       0.72      0.72      0.72       231\n",
      "\n",
      "the accuracy for RandomForestClassifier(n_estimators=1000) is 0.7186147186147186\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "rf_model(X, y, RandomForestClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "mlenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
